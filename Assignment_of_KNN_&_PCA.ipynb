{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment of KNN & PCA**"
      ],
      "metadata": {
        "id": "csMkBiWqX8OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?"
      ],
      "metadata": {
        "id": "yjwHHlvfYRhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## **K-Nearest Neighbors (KNN)**\n",
        "\n",
        ">KNN is a **supervised machine-learning algorithm** used for **classification** and **regression**. It predicts outputs based on the idea that **similar data points are close to each other** in feature space.\n",
        "\n",
        "> It is a **lazy learner**â€”meaning it does not build a model during training. Instead, it stores the training data and makes predictions only when needed.\n",
        "\n",
        "\n",
        ">  **How KNN Works**\n",
        "\n",
        "> For a new input point:\n",
        ">1. Compute its distance to all points in the training dataset.\n",
        "2. Select the **K closest points** (neighbors).\n",
        "3. Make a prediction using those neighbors.\n",
        "\n",
        "> Common distance metrics:\n",
        ">* Euclidean (most common)\n",
        ">* Manhattan\n",
        ">* Minkowski\n",
        ">* Cosine similarity\n",
        "\n",
        "\n",
        ">##  **KNN for Classification**\n",
        "Steps:\n",
        ">1. Find the **K nearest neighbors** of the new point.\n",
        "2. Each neighbor â€œvotesâ€ for its class label.\n",
        "3. The class with the **majority vote** becomes the prediction.\n",
        "\n",
        "Example:\n",
        "If K = 5 and among the neighbors 3 are Class A and 2 are Class B â†’ predicted class = **A**.\n",
        "\n",
        "\n",
        ">##  **KNN for Regression**\n",
        "Steps:\n",
        ">1. Find the **K nearest neighbors**.\n",
        "2. Instead of voting, take the **average value** (or weighted average) of their numerical labels.\n",
        "\n",
        "Example:\n",
        "Neighbor values = 10, 12, 14 â†’ prediction = (10 + 12 + 14) / 3 = **12**.\n",
        "\n",
        ">**Weighted regression:** closer points have more influence by using weights like 1/distance.\n",
        "\n",
        "\n",
        "> ###  **Choosing the Value of K**\n",
        ">* **Small K** â†’ more complex, risk of overfitting.\n",
        ">* **Large K** â†’ smoother, risk of underfitting.\n",
        ">* Typically selected using cross-validation.\n",
        "\n",
        "\n",
        ">### **Strengths of KNN**\n",
        ">* Very simple, easy to understand.\n",
        ">* Works well for small/medium datasets.\n",
        ">* Handles non-linear decision boundaries.\n",
        ">* No training time needed.\n",
        "\n",
        "\n",
        "\n",
        ">###  **Weaknesses of KNN**\n",
        ">* Slow prediction on large datasets (computes many distances).\n",
        ">* Sensitive to irrelevant features and differences in scale â†’ requires normalization.\n",
        ">* Performs poorly in very high-dimensional spaces (curse of dimensionality).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pb6UzX7TYJrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is the Curse of Dimensionality and how does it affect KNN performance?"
      ],
      "metadata": {
        "id": "Jq27bqOAa7U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">##  **Curse of Dimensionality**\n",
        ">The **Curse of Dimensionality** refers to a set of problems that arise when data has **too many features (dimensions)**. As the number of dimensions increases:\n",
        ">* Data becomes **sparse** (spread out).\n",
        ">* Distances between points become **less meaningful**.\n",
        ">* Models that rely on distance or neighborhood relationships become less effective.\n",
        "\n",
        ">In simple terms:\n",
        "**High-dimensional space makes it harder to find â€œnearbyâ€ points.**\n",
        "\n",
        "\n",
        ">##  **Why Does This Happen?**\n",
        ">When you add more dimensions:\n",
        ">1. **Space grows exponentially**, but the amount of data does not.\n",
        "    > * You would need exponentially more data to cover the space effectively.\n",
        "    \n",
        ">2. **All points become far apart.**\n",
        "    >* The difference between the nearest and farthest neighbors shrinks.\n",
        "\n",
        ">3. **Distance metrics stop working well.**\n",
        "     >* Euclidean distance becomes less discriminative.\n",
        "\n",
        "\n",
        ">##  **How the Curse of Dimensionality Affects KNN**\n",
        "\n",
        ">KNN heavily depends on **distance** to find the closest neighbors. In high dimensions, this becomes problematic.\n",
        ">### **1. Distances lose meaning**\n",
        ">When dimensionality is high:\n",
        ">* Nearest neighbors are **not much closer** than farthest neighbors.\n",
        ">* KNN struggles to identify truly similar points.\n",
        "\n",
        "This leads to **poor classification and regression accuracy**.\n",
        ">### **2. Increased risk of overfitting**\n",
        ">With many irrelevant features:\n",
        ">* Noise dominates genuine patterns.\n",
        ">* KNN may pick misleading neighbors because distance is distorted.\n",
        ">### **3. Computational cost increases**\n",
        ">More dimensions mean:\n",
        ">* More distance calculations\n",
        ">* Each distance computation becomes more expensive\n",
        ">* Prediction time becomes slow\n",
        "\n",
        "Since KNN is a lazy learner, this slows down the algorithm significantly.\n",
        ">### **4. Need for feature scaling and dimensionality reduction**\n",
        ">To combat high-dimensional issues, techniques like:\n",
        ">* **PCA (Principal Component Analysis)**\n",
        ">* **Feature selection (e.g., removing irrelevant features)**\n",
        ">* **Normalization/standardization**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4vstSfVgazxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is Principal Component Analysis (PCA)? How is it different from feature selection?"
      ],
      "metadata": {
        "id": "RLemiNh4AXay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">##  **Principal Component Analysis (PCA) :-**\n",
        "**PCA** is an **unsupervised dimensionality reduction technique** that transforms high-dimensional data into a smaller set of **new variables** called **principal components**.\n",
        "\n",
        ">These components:\n",
        ">* Are **linear combinations** of the original features\n",
        ">* Capture the **maximum variance** in the data\n",
        ">* Are **uncorrelated** with each other\n",
        ">* Are ordered:\n",
        ">   * 1st component â†’ most variance\n",
        ">   * 2nd component â†’ second-most variance\n",
        ">   * ...and so on\n",
        "\n",
        ">PCA is mainly used to:\n",
        ">* Reduce dimensionality (to combat the curse of dimensionality)\n",
        ">* Remove noise\n",
        ">* Visualize high-dimensional data\n",
        ">* Speed up algorithms that struggle with many features\n",
        "\n",
        "\n",
        ">##  **How PCA Works (Conceptually)**\n",
        ">1. Standardize the data.\n",
        "2. Compute the covariance matrix.\n",
        "3. Find its eigenvalues and eigenvectors.\n",
        "4. Sort components by variance explained.\n",
        "5. Select the top *k* components.\n",
        "6. Transform data into the new component space.\n",
        "\n",
        "The transformed features are **not original features**, but new axes that best describe the data's structure.\n",
        "\n",
        "\n",
        ">##  **How PCA Differs from Feature Selection**\n",
        "PCA is a **feature extraction** method, not a selection method.\n",
        "| Aspect               | PCA (Feature Extraction)                                                   | Feature Selection                             |\n",
        "| -------------------- | -------------------------------------------------------------------------- | --------------------------------------------- |\n",
        "| **What it does**     | Creates new features (principal components) by combining original features | Chooses a subset of the existing features     |\n",
        "| **Output features**  | New, transformed, uncorrelated components                                  | Original features only                        |\n",
        "| **Interpretability** | Low â€” components are combinations of variables                             | High â€” uses actual features                   |\n",
        "| **Type**             | Unsupervised                                                               | Usually supervised (based on target variable) |\n",
        "| **Goal**             | Reduce dimensionality while preserving variance                            | Remove irrelevant or redundant features       |\n",
        "| **Changes data?**    | Yes â€” transforms it                                                        | No â€” keeps original feature meanings          |\n",
        "\n",
        "\n",
        ">###  Example\n",
        ">Suppose you have 10 correlated features.\n",
        ">* **PCA** may create 3 new components that capture 95% of the variance.\n",
        ">* **Feature selection** may choose only 3 of the original 10 features.\n",
        "\n",
        "Both reduce dimensionality, but in very different ways.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Gev942_APfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What are eigenvalues and eigenvectors in PCA, and why are they important?"
      ],
      "metadata": {
        "id": "bwsQQ6egCUqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## **Eigenvalues and Eigenvectors in PCA :-**\n",
        "In PCA, we compute the **covariance matrix** of the data and then find its **eigenvalues** and **eigenvectors**.\n",
        "\n",
        ">### **Eigenvectors**\n",
        ">* Define the **directions** of the new feature axes (principal components).\n",
        ">* Each eigenvector represents a direction in which the data varies.\n",
        ">* They are orthogonal (uncorrelated) to each other.\n",
        "\n",
        ">### **Eigenvalues**\n",
        ">* Tell us **how much variance** each eigenvector (component) captures.\n",
        ">* Larger eigenvalue â†’ more important principal component.\n",
        ">* They allow us to rank components by significance.\n",
        "\n",
        ">In short:\n",
        ">* **Eigenvectors = directions of maximum variance**\n",
        ">* **Eigenvalues = amount of variance in those directions**\n",
        "\n",
        "\n",
        ">##  **Eigenvalues and Eigenvectors Importance in PCA :-**\n",
        ">### **1. They determine the principal components**\n",
        "PCA picks the top *k* eigenvectors (based on largest eigenvalues).\n",
        "These become the new axes after transformation.\n",
        ">### **2. They help reduce dimensionality**\n",
        "Eigenvalues indicate how much information (variance) each component contains.  \n",
        "Example:\n",
        "If the first 2 components capture 95% of total variance, we can safely reduce from, say, 20 dimensions to 2.\n",
        ">### **3. They ensure uncorrelated components**\n",
        "Because eigenvectors of the covariance matrix are orthogonal:\n",
        ">* Principal components do not overlap in the information they describe.\n",
        ">* Models often perform better with uncorrelated features.\n",
        ">### **4. They help remove noise**\n",
        "Small eigenvalues correspond to directions with very little variance, often noise.\n",
        ">By removing components with small eigenvalues, PCA:\n",
        ">* Simplifies data\n",
        ">* Enhances signal\n",
        ">* Reduces overfitting\n",
        "\n",
        ">###  Example\n",
        ">Imagine your data lies mostly along a diagonal line on a 2D plane:\n",
        ">1. PCA finds the eigenvector aligned with this line â†’ **1st principal component**.\n",
        ">2. The eigenvalue for this direction is large â†’ lots of variance.\n",
        ">3. A second eigenvector perpendicular to the line captures little variance (small eigenvalue).\n",
        "\n",
        "So you keep the first component and drop the second â†’ dimensionality reduced from 2D to 1D.\n"
      ],
      "metadata": {
        "id": "M0O13GVBCNxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  5. How do KNN and PCA complement each other when applied in a single pipeline?"
      ],
      "metadata": {
        "id": "bLH2SNLCEeGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## **How KNN and PCA Complement Each Other**\n",
        "KNN and PCA are commonly used together because PCA helps address several weaknesses of KNN. The combination improves both **accuracy** and **efficiency**.\n",
        ">###  **1. PCA reduces dimensionality â†’ makes KNN more effective**\n",
        "KNN suffers from the **curse of dimensionality** because distance becomes meaningless when there are too many features.\n",
        "PCA:\n",
        ">* Reduces the number of features\n",
        ">* Removes noise\n",
        ">* Keeps the most informative directions of variance.\n",
        "\n",
        ">This makes distance-based algorithms like KNN **more reliable**.\n",
        "\n",
        ">###  **2. PCA removes correlated and redundant features**\n",
        "If many features are correlated:\n",
        ">* KNN may overemphasize those dimensions\n",
        ">* Distances can become distorted\n",
        "\n",
        "PCA transforms the data into **uncorrelated (orthogonal)** components, which leads to more meaningful distance calculations.\n",
        "This directly improves KNN performance.\n",
        "\n",
        ">###  **3. PCA reduces noise â†’ KNN makes better neighbor choices**\n",
        "KNN has no internal mechanism for ignoring noise, since it relies entirely on raw distances.\\\n",
        "PCA removes components with very small variance (often noise), helping KNN:\n",
        ">* Avoid misleading neighbors\n",
        ">* Improve generalization\n",
        ">* Reduce overfitting\n",
        "\n",
        ">###  **4. PCA improves KNN speed**\n",
        "KNN is slow at prediction time because it must compute distances to all points.\\\n",
        "Reducing dimensions via PCA:\n",
        ">* Decreases computational cost\n",
        ">* Makes KNN faster\n",
        ">* Reduces memory usage\n",
        "\n",
        "This is especially important for large datasets.\n",
        ">###  **5. PCA enables better visualization before applying KNN**\n",
        "Using PCA to project high-dimensional data into 2D or 3D helps you:\n",
        ">* Visualize class separation\n",
        ">* Detect clusters\n",
        ">* Identify whether KNN is appropriate\n",
        "\n",
        "This supports better model design.\n",
        ">###  **6. PCA + KNN is a common ML pipeline**\n",
        "A typical pipeline looks like:\n",
        ">1. **Scale the data** (important for both PCA and KNN)\n",
        "2. **Apply PCA** to reduce to *k* principal components\n",
        "3. **Use KNN** on the transformed feature space\n",
        "\n",
        ">This pipeline often provides:\n",
        ">* Higher accuracy\n",
        ">* Better generalization\n",
        ">* Faster prediction\n",
        ">* More stable distance metrics\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fw0s_40vEbo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "C5hxh1FfHZsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()        # loads dataset directly from sklearn\n",
        "X = data.data             # features\n",
        "y = data.target           # labels\n",
        "\n",
        "# Split into training/testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
        "\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(\"Accuracy WITHOUT Scaling:\", accuracy_no_scale)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy WITH Scaling:\", accuracy_with_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X8WDf9OHney",
        "outputId": "16126432-112c-4584-894f-941a94e93615"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.7222222222222222\n",
            "Accuracy WITH Scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "FHuX8RSdH3Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eemuBXlDH_NJ",
        "outputId": "6f401b19-680a-4d94-f3dc-a87b0af7e5bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "jLCw2d5sIIfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy on ORIGINAL scaled dataset:\", accuracy_original)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy on PCA (2 components) dataset:\", accuracy_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg5lXbGnIUwH",
        "outputId": "ded4465a-b9cd-4a70-ff4b-c07c11f2801f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on ORIGINAL scaled dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 components) dataset: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "BWeO1j-fIic2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def evaluate_knn(metric_name, **knn_kwargs):\n",
        "    knn = KNeighborsClassifier(**knn_kwargs)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n--- KNN (metric = {metric_name}) ---\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names, digits=4))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
        "                       index=[f\"true_{t}\" for t in target_names],\n",
        "                       columns=[f\"pred_{t}\" for t in target_names]))\n",
        "    return acc\n",
        "\n",
        "acc_euclidean = evaluate_knn(\"euclidean\", n_neighbors=5, metric=\"minkowski\", p=2)\n",
        "\n",
        "acc_manhattan = evaluate_knn(\"manhattan\", n_neighbors=5, metric=\"manhattan\")\n",
        "\n",
        "print(\"\\nSummary of accuracies:\")\n",
        "print(f\"Euclidean (p=2) : {acc_euclidean:.4f}\")\n",
        "print(f\"Manhattan (L1)   : {acc_manhattan:.4f}\")\n",
        "\n",
        "ks = [1,3,5,7,9,11]\n",
        "results = []\n",
        "for k in ks:\n",
        "    knn_e = KNeighborsClassifier(n_neighbors=k, metric=\"minkowski\", p=2).fit(X_train_scaled, y_train)\n",
        "    knn_m = KNeighborsClassifier(n_neighbors=k, metric=\"manhattan\").fit(X_train_scaled, y_train)\n",
        "    acc_e = accuracy_score(y_test, knn_e.predict(X_test_scaled))\n",
        "    acc_m = accuracy_score(y_test, knn_m.predict(X_test_scaled))\n",
        "    results.append((k, acc_e, acc_m))\n",
        "\n",
        "print(\"\\nAccuracy by k:\")\n",
        "print(pd.DataFrame(results, columns=[\"k\", \"euclidean_acc\", \"manhattan_acc\"]).set_index(\"k\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ri-bNwJCpX",
        "outputId": "7428cdc3-f698-4019-c6b2-8cd4a8e6eac1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- KNN (metric = euclidean) ---\n",
            "Accuracy: 0.9444\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0     1.0000    1.0000    1.0000        18\n",
            "     class_1     1.0000    0.8571    0.9231        21\n",
            "     class_2     0.8333    1.0000    0.9091        15\n",
            "\n",
            "    accuracy                         0.9444        54\n",
            "   macro avg     0.9444    0.9524    0.9441        54\n",
            "weighted avg     0.9537    0.9444    0.9448        54\n",
            "\n",
            "Confusion Matrix:\n",
            "              pred_class_0  pred_class_1  pred_class_2\n",
            "true_class_0            18             0             0\n",
            "true_class_1             0            18             3\n",
            "true_class_2             0             0            15\n",
            "\n",
            "--- KNN (metric = manhattan) ---\n",
            "Accuracy: 0.9815\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0     1.0000    1.0000    1.0000        18\n",
            "     class_1     1.0000    0.9524    0.9756        21\n",
            "     class_2     0.9375    1.0000    0.9677        15\n",
            "\n",
            "    accuracy                         0.9815        54\n",
            "   macro avg     0.9792    0.9841    0.9811        54\n",
            "weighted avg     0.9826    0.9815    0.9816        54\n",
            "\n",
            "Confusion Matrix:\n",
            "              pred_class_0  pred_class_1  pred_class_2\n",
            "true_class_0            18             0             0\n",
            "true_class_1             0            20             1\n",
            "true_class_2             0             0            15\n",
            "\n",
            "Summary of accuracies:\n",
            "Euclidean (p=2) : 0.9444\n",
            "Manhattan (L1)   : 0.9815\n",
            "\n",
            "Accuracy by k:\n",
            "    euclidean_acc  manhattan_acc\n",
            "k                               \n",
            "1        0.962963       0.962963\n",
            "3        0.944444       0.962963\n",
            "5        0.944444       0.981481\n",
            "7        0.944444       0.981481\n",
            "9        0.962963       0.981481\n",
            "11       0.962963       0.962963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "###Explain how you would:\n",
        "###  -   Use PCA to reduce dimensionality\n",
        "### -  Decide how many components to keep\n",
        "### -  Use KNN for classification post-dimensionality reduction\n",
        "### - Evaluate the model\n",
        "### - Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n"
      ],
      "metadata": {
        "id": "0wfQEETXJd1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## ðŸ”¹ **1. Using PCA to Reduce Dimensionality**\n",
        "Gene expression datasets often contain **thousands of genes**. Many of these:\n",
        ">* Are correlated\n",
        ">* Contain noise\n",
        ">* Provide redundant information\n",
        "\n",
        ">To reduce dimensionality:\n",
        ">1. **Standardize the data** (PCA requires scaling)\n",
        "2. Apply PCA to transform the original features into a smaller number of **principal components**\n",
        "3. These components capture the majority of biological signal while reducing noise\n",
        "\n",
        ">PCA helps combat:\n",
        ">* Overfitting\n",
        ">* Noise sensitivity\n",
        ">* The curse of dimensionality\n",
        "\n",
        "\n",
        ">## ðŸ”¹ **2. Deciding How Many Components to Keep**\n",
        "We select the number of principal components using:\n",
        ">### **Explained Variance Ratio**\n",
        "Choose enough components to capture **90â€“95% of total variance**, ensuring minimal information loss.\n",
        ">### **Scree / Elbow Plot**\n",
        "Pick the point where adding more components yields diminishing returns.\n",
        ">###  **Cross-validation**\n",
        "Evaluate classification accuracy for different numbers of components and choose the number that performs best.\\\n",
        "This ensures that PCA keeps only the **most meaningful biological patterns**.\n",
        "\n",
        "\n",
        ">## ðŸ”¹ **3. Using KNN for Classification After PCA**\n",
        "After PCA transforms the dataset:\n",
        ">* The data becomes **lower-dimensional, noise-reduced, and uncorrelated**.\n",
        ">* KNN becomes more effective because distances are now meaningful.\n",
        ">* Overfitting risk decreases dramatically.\n",
        "\n",
        ">Steps:\n",
        ">1. Train a **KNN classifier** on the PCA-transformed features.\n",
        "2. Tune **k** using cross-validation.\n",
        "3. Predict cancer type for new patients.\n",
        "\n",
        ">KNN is simple, interpretable, and benefits greatly from PCA.\n",
        "\n",
        ">## ðŸ”¹ **4. Model Evaluation**\n",
        "Use:\n",
        ">### **Stratified trainâ€“test split or cross-validation**\n",
        "Ensures all cancer types are represented.\n",
        ">###  **Metrics**\n",
        ">* Accuracy\n",
        ">* Precision, recall, F1-score\n",
        ">* Confusion matrix\n",
        ">* ROC-AUC (one-vs-rest for multiclass)\n",
        ">### **Repeated cross-validation**\n",
        "Because sample sizes are small, repeating K-fold CV yields more stable estimates.\n",
        "\n",
        "\n",
        ">## ðŸ”¹ **5. Justification to Stakeholders (Real-World Biomedical Context)**\n",
        "This PCA â†’ KNN pipeline is a strong choice because:\n",
        ">###  **Addresses overfitting in high-dimensional biomedical data**\n",
        "PCA extracts the core biological signal and removes noise.\n",
        ">###  **Produces stable, reproducible results**\n",
        "PCA reduces variance, ensuring the model doesnâ€™t learn patient-specific noise.\n",
        ">###  **Improves interpretability**\n",
        "Principal components can be mapped back to gene groups, allowing domain experts to study which pathways drive classification.\n",
        ">###  **Computationally efficient**\n",
        "KNN on reduced PCA features is much faster and more scalable.\n",
        ">###  **Widely used in genomics**\n",
        "Dimensionality reduction + distance-based methods are standard in cancer subtype analysis, clustering, and biomarker discovery.\n",
        ">###  **Transparent & trusted**\n",
        "Unlike black-box deep learning, PCA + KNN is interpretable and easy to validate in clinical settings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-n07lz02JJcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA + KNN Pipeline for High-Dimensional Data\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Number of PCA components retained:\", pca.n_components_)\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy after PCA + KNN:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_A97MxZLiDO",
        "outputId": "3a057d02-b4a1-4566-dd30-15451e3bf85b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PCA components retained: 10\n",
            "Explained variance ratio: [0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019]\n",
            "\n",
            "Accuracy after PCA + KNN: 0.9629629629629629\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.88      1.00      0.94        15\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.96      0.97      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n"
          ]
        }
      ]
    }
  ]
}